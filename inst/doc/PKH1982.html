<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Victor Navarro" />


<title>PKH1982</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>






<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
margin-bottom: 0em;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">PKH1982</h1>
<h4 class="author">Victor Navarro</h4>



<div id="the-mathematics-behind-pkh1982" class="section level1">
<h1>The mathematics behind PKH1982</h1>
<p>Another departure from global error term models such as RW1972 <span class="citation">(Rescorla &amp; Wagner, 1972)</span>, the PKH1982 model
<span class="citation">(Pearce et al., 1982)</span> does not use an
error term for learning excitatory associations (but does for inhibitory
associations), and ties stimulus associability (<span class="math inline">\(\alpha\)</span>) to absolute global prediction
error.</p>
<p><em>note: The implementation of this model closely follows the
technical note from the <a href="https://www.cal-r.org/index.php?id=PHsim">CAL-R</a> group where
possible. Divergences are noted.</em></p>
<div id="generating-expectations" class="section level2">
<h2>1 - Generating expectations</h2>
<p>Let <span class="math inline">\(v_{k,j}\)</span> denote the
excitatory strength from stimulus <span class="math inline">\(k\)</span>
to stimulus <span class="math inline">\(j\)</span>, and <span class="math inline">\(v_{k,\overline j}\)</span> the inhibitory strength
from stimulus <span class="math inline">\(k\)</span> to stimulus <span class="math inline">\(j\)</span> (effectively, a “no j” representation).
On any given trial, the net expectation of stimulus <span class="math inline">\(j\)</span>, <span class="math inline">\(e_j\)</span>, is given by:</p>
<p><span class="math display">\[
\tag{Eq.1}
e_j = \sum_{k}^{K}x_k v_{k,j} - \sum_{k}^{K}x_k v_{k,\overline j}
\]</span></p>
<p>where <span class="math inline">\(x_k\)</span> denotes the presence
(1) or absence (0) of stimulus <span class="math inline">\(k\)</span>,
and the set <span class="math inline">\(K\)</span> represents all
stimuli in the design.</p>
</div>
<div id="learning-associations" class="section level2">
<h2>2 - Learning associations</h2>
<p>Changes to the excitatory and inhibitory associations between stimuli
are given by:</p>
<p><span class="math display">\[
\tag{Eq.2a}
\Delta v_{i,j} = \delta_jx_i \beta_{ex,j} \alpha_i \lambda_j
\]</span> <span class="math display">\[
\tag{Eq.2b}
\Delta v_{i,\overline j} = x_i \beta_{in,j}
\alpha_i  |\overline{\lambda_j}|
\]</span></p>
<p>where <span class="math inline">\(\beta_{ex,j}\)</span> and <span class="math inline">\(\beta_{in,j}\)</span> represent learning rates for
excitatory and inhibitory associations, respectively, as determined by
stimulus <span class="math inline">\(j\)</span>, <span class="math inline">\(\alpha_i\)</span> is the associability of stimulus
<span class="math inline">\(i\)</span>, respectively, and <span class="math inline">\(\lambda_j\)</span> and <span class="math inline">\(\overline {\lambda_j}\)</span> are the excitatory
asymptote and the overexpectation of stimulus <span class="math inline">\(j\)</span>, respectively.</p>
<p>Importantly, <span class="math inline">\(\delta_j\)</span> in Eq.2a
is a parameter that is equal to 1 if the expectation of stimulus <span class="math inline">\(j\)</span>, is lower than its excitatory asymptote
(i.e., <span class="math inline">\(e_j &lt; \lambda_j\)</span>), but 0
if not. This implies that the model stops strengthening <span class="math inline">\(v_{i,j}\)</span> if the expectation of <span class="math inline">\(j\)</span> is higher than its excitatory
asymptote.</p>
<p>As mentioned in the introductory note, the PKH1982 model does not
learn excitatory associations via correction error. However, the model
<strong>does</strong> learn inhibitory associations via correction
error, as the overexpectation term above, <span class="math inline">\(\overline
{\lambda_j}\)</span> is equal to <span class="math inline">\(min(\lambda_j - e_j, 0)\)</span>, where <span class="math inline">\(min\)</span> is the minimum function. This implies
<span class="math inline">\(\overline {\lambda_j}\)</span> only takes
non-zero values when the expectation of <span class="math inline">\(j\)</span> is higher than its intensity on the
trial (<span class="math inline">\(\lambda_j\)</span>).</p>
</div>
<div id="learning-to-attend" class="section level2">
<h2>3 - Learning to attend</h2>
<p>The associability parameter <span class="math inline">\(\alpha_i\)</span> changes completely from trial to
trial as a function of learning (note the lack of <span class="math inline">\(\Delta\)</span> below), with the change being
equal to the difference of the absolute global error, via:</p>
<p><span class="math display">\[
\tag{Eq.3}
\alpha_{i} = x_i \sum_{j}^{K}\gamma_j(|\lambda_j - e_j|)
\]</span> where <span class="math inline">\(\gamma_j\)</span> denotes
the contribution of the prediction error based on the jth stimulus. In
this regard, it is important to note that <span class="citation">Pearce
et al. (1982)</span> did not extend their model to account for the
predictive power of within-compound associations, yet the implementation
of the model in this package does. This can sometimes result in
unexpected behaviour, and as such, Eq. 3 above includes the extra
parameter <span class="math inline">\(\gamma_j\)</span> (defaulting to
1/K) that denotes whether the expectation of stimulus <span class="math inline">\(j\)</span> contributes to attentional learning. As
such, the user can set these parameters manually to reflect the
contribution of the different experimental stimuli. For example, in a
simple “AB&gt;(US)” design, setting <span class="math inline">\(\gamma_{US}\)</span> = 1 and <span class="math inline">\(\gamma_{A} =
\gamma_{B} = 0\)</span> leads to the behavior of the original model.</p>
<p>The PKH1982 model improves upon the <span class="citation">Pearce
&amp; Hall (1980)</span> model by adding an extra parameter that
controls the rate at which associability changes. If we qualify the
changes in associability described by Eq.3 via <span class="math inline">\(\alpha_{i}^{n}\)</span> (meaning they happened
after trial <span class="math inline">\(n\)</span>), then we can
quantify the total associability of stimulus <span class="math inline">\(i\)</span> after trial <span class="math inline">\(n\)</span> via:</p>
<p><span class="math display">\[
\tag{Eq.4}
\alpha_{i}^{n} =
\begin{cases}
  (1-\theta_i) \alpha_{i}^{n-1} + \theta_i\alpha_{j}^n &amp;\text{, if }
x_i = 1\\
  \alpha_{i}^{n} &amp; \text{, otherwise}
\end{cases}
\]</span> where <span class="math inline">\(\theta_i\)</span> is a
parameter determining both the rate at which associability decays (via
<span class="math inline">\(1-\theta_i\)</span>), and the rate at which
increments in attention occur. Note that changes in associability only
apply to stimuli presented on the trial (i.e., <span class="math inline">\(x_i = 1\)</span>); attention to absent stimuli
remains unchanged.</p>
</div>
<div id="generating-responses" class="section level2">
<h2>4 - Generating responses</h2>
<p>There is no specification of response-generating mechanisms in
PKH1982. However, the simplest response function that can be adopted is
the identity function on stimulus expectations. If so, the responses
reflecting the nature of <span class="math inline">\(j\)</span>, <span class="math inline">\(r_j\)</span>, are given by:</p>
<p><span class="math display">\[
\tag{Eq.5}
r_j = e_j
\]</span></p>
<div id="references" class="section level3 unnumbered">
<h3 class="unnumbered">References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-pearce_model_1980" class="csl-entry">
Pearce, J. M., &amp; Hall, G. (1980). A model for <span>Pavlovian</span>
learning: <span>Variations</span> in the effectiveness of conditioned
but not of unconditioned stimuli. <em>Psychological Review</em>,
<em>87</em>, 532–552. <a href="https://doi.org/10.1037/0033-295X.87.6.532">https://doi.org/10.1037/0033-295X.87.6.532</a>
</div>
<div id="ref-pearce_predictive_1982" class="csl-entry">
Pearce, J. M., Kaye, H., &amp; Hall, G. (1982). Predictive accuracy and
stimulus associability: <span>Development</span> of a model for
<span>Pavlovian</span> conditioning. In <em>Quantitative analyses of
behavior: <span>Acquisition</span></em> (Vol. 3, pp. 241–255).
Ballinger.
</div>
<div id="ref-rescorla_theory_1972" class="csl-entry">
Rescorla, R. A., &amp; Wagner, A. R. (1972). A theory of
<span>Pavlovian</span> conditioning: <span>Variations</span> in the
effectiveness of reinforcement and nonreinforcement. In A. H. Black
&amp; W. F. Prokasy (Eds.), <em>Classical conditioning <span>II</span>:
<span>Current</span> research and theory.</em> (pp. 64–69).
Appleton-Century-Crofts.
</div>
</div>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
